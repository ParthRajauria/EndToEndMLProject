################################ End to End ML Project ##########################

Topics to Cover

0. Set up project with github.
1. Data Ingestion
2. Data Transformation
3. Model Trainer
4. Model Evaluation
5. Model Deployment


CI/CD Pipelines - Github Actions
Deployment - AWS

############################ What Did I Learn ########################
Point 1 - How to create a modular code design and how to design ml workflow.
Point 2 - How to commit files on github using vs code and terminal.
Point 3 - How to train models using functions.
Point 4 - How to make data ingestion and data transformation pipelines.
Point 5 - How to deploy your repository using CI/CD pipelines to AWS and Azure. Although we need to watch the youtube video again to do this deeployment for our project.


######################### Steps To Commit file in github from vs code ##############
Step 1 - First connect to your github repository through vs code.
Step 2 - After modifying the files type git status in terminal to check the changes in your repository.
Step 3 - Then type git add . to add tall the files in the current repository.
Step 4 - Then type git commit -m "Release Notes" to commit the file in local repository
Step 5 - Then type git push -u origin main to release the file from local repository (origin) to main repository.


########################## Miscellaneous Details #################################
* We will be using AWS Elastic beanstalk to deploy our code. 
* We use CodePipeline feature of AWS for Continuous Delivery (CD) pipeline.
* Idea of GithubActions is that any code which comes to the github repo should be automatically updated in our deployment server.
* To create virtual environment use conda create -p venv python==3.8 -y
* To activate this environment type conda activate venv


######## Steps in each Tutorial    #########


#####  Tutorial 1 - Github and Code Setup    ######

Step 1 - Create a repository in github
Step 2 - Go to vs code in your project directory, open terminal and create a virtual environment
Step 3 - Sync the github repository with vs code. So that we can commit all our code to github easily. 
Step 4 - Create a gitignore file in github. We use this so that the files/libraries that are not committed on github can be removed and can free up space and increase computational power. Use git pull to pull the files from github repository to vs code.
Step 5 - Create setup.py file. This file will be responsible for creating our machine learning application as a package. And we can use this package later for any other project.
Step 6 - Create a folder called src, and create a file in this src folder called __init__.py
Step 7 - Now install all the dependencies by running pip install -r requirements.txt


##### Tutorial 2 - Project Structure, Logging, and Exception Handling.

Step 1 - create a components folder under src folder. In that, create a __init__.py file. Components will contain all the modules, like data ingestion. EDA, etc. 
Step 2 - create a pipeline directory. In that have training and testing pipeline. 
Step 3 - Create 3 more files, utils.py (for writing generic functions that are used everywhere), exception.py (for exception handling), and logger.py (for data logging)
Step 4 - Write code on exception.py file. 

#### Tutorial 3 - Project Problem Statement, EDA, and Model Training. 

Step 1 - Create repository notebook.
Step 2 - Copy data and EDA notebooks and Model Training notebook from krish naik repository to this repo. 
Step 3 - Execute EDA notebook one by one.

### Tutorial 4 - Data Ingestion implementation line by line.
Step 1 - Work on the data_ingestion.py file present inside src/components directory. Include data ingestion pipeline with proper logs.

### Tutorial 5 - Data Transformation implementation using pipeline.
Step 1 - Add the code in data_transformation.py file.
Step 2 - Add the code of saving object in utlis.py
Step 3 - Add the code of calling data_transformation in data_ingestion.py file.

### Tutorial 6 & 7 - Model Training and Model Evaluating Component
Step 1 - Add the code in model_trainer.py
Step 2 - Add the required function in utils.py
Step 3 - Add the required code in data_ingestion.py

### Tutorial 8 - Create Prediction pipeline using Flask Web App
Step 1 - Create app.py file and develop the flask app code for it.
Step 2 - Add code in prediction_pipeline.py file.
Step 3 - Run the flask app and open the url in web browser 127.0.0.1:5000
Step 4 - Type all the feature values and observe the model prediction.

### Tutorial 9 - Deployment in AWS Cloud using CI/CD Pipelines.
Step 1 - We need to make two configuration changes - create a .ebextensions folder for elastic beanstalk, then create a python.config file inside it.
Note:- We use CodePipeline feature of AWS for Continuous Delivery (CD) pipeline.

### Tutorial 10 - Deployment of ML Applications in Azure Cloud using Github Actions
Step 1 - Make account in Azue and follow the steps mentioned in this video:- https://www.youtube.com/watch?v=SkzmbeYCtiU&list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG&index=11
